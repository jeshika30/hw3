{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "from time import sleep\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for debugging\n",
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Check Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in to your Hugging Face account.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a0dc394d024577ae0a2896926b8db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Model Loading ---\n",
    "# We'll run our models using Hugging Face's transformers library on HPC/ Google Colab/ Lightning.ai\n",
    "# The Llama models are gated, meaning you must request access on their Hugging Face pages.\n",
    "# Once you have access, you need to log in here to download the model weights.\n",
    "\n",
    "# Run this command in your terminal when you are running this notebook for the 1st time\n",
    "# git config --global credential.helper store\n",
    "\n",
    "print(\"Please log in to your Hugging Face account.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our primary model for most of the assignment.\n",
    "model_id_1 = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec086fbda969466e99d0fcca68edfcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt, student_configs, post_processing_fn, model_obj, tokenizer_obj, debug=False):\n",
    "    \"\"\"\n",
    "    Generates a response using the provided local Hugging Face model and tokenizer.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input prompt\n",
    "    inputs = tokenizer_obj(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    hf_configs = student_configs.copy()\n",
    "    if 'max_tokens' in hf_configs:\n",
    "        # `generate` uses `max_new_tokens` to specify the length of the output\n",
    "        hf_configs['max_new_tokens'] = hf_configs.pop('max_tokens')\n",
    "    if 'stop' in hf_configs:\n",
    "        del hf_configs['stop'] # Stop sequences are handled differently; we'll ignore for simplicity\n",
    "\n",
    "    # 2. Generate output tokens\n",
    "    outputs = model_obj.generate(**inputs, **hf_configs).to(device)\n",
    "    \n",
    "    # 3. Decode the generated tokens back to a string\n",
    "    # We slice the output to only get the newly generated text, not the original prompt\n",
    "    result_new = tokenizer_obj.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    dprint(\"************ Prompt ************\", debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint(\"\\n************ Raw Response ************\", debug)\n",
    "    dprint(result_new, debug)\n",
    "\n",
    "    # 4. Apply post-processing to extract the final answer\n",
    "    final_output = post_processing_fn(result_new)\n",
    "    \n",
    "    dprint(\"\\n************ Final Output ************\", debug)\n",
    "    dprint(final_output, debug)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    \"\"\"Generates two random integers within a specified range.\"\"\"\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, model_obj, tokenizer_obj, n_sample=30,\n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None,\n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               debug=False):\n",
    "    \"\"\"\n",
    "    Tests a language model's addition performance over a range of numbers.\n",
    "\n",
    "    Args:\n",
    "        added_prompt (tuple): A tuple containing the prefix and suffix for the prompt.\n",
    "        prompt_configs (dict): Configuration parameters for the model's generate function.\n",
    "        rng (numpy.random.Generator): A random number generator instance.\n",
    "        model_obj (transformers.PreTrainedModel): The loaded Hugging Face model object.\n",
    "        tokenizer_obj (transformers.PreTrainedTokenizer): The loaded Hugging Face tokenizer object.\n",
    "        n_sample (int): The number of random pairs to generate if fixed_pairs is None.\n",
    "        lower_bound (int): The lower bound for number generation.\n",
    "        upper_bound (int): The upper bound for number generation.\n",
    "        fixed_pairs (list, optional): A list of specific integer tuples to test.\n",
    "        pre_processing (function): A function to apply to the input string before prompting.\n",
    "        post_processing (function): A function to extract the integer answer from the model's output.\n",
    "        debug (bool): If True, prints detailed debugging information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics (res, acc, mae, prompt_length).\n",
    "    \"\"\"\n",
    "    # --- Lists for storing results ---\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    \n",
    "    # --- Determine the test cases ---\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    \n",
    "    for v in tqdm(iterations):\n",
    "        if fixed_pairs is None:\n",
    "            # Generate two new numbers if no fixed pairs are provided\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            # Use the provided fixed pairs\n",
    "            int_a, int_b = v\n",
    "            \n",
    "        # --- Construct the prompt for two numbers ---\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        \n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        \n",
    "        # --- Get the model's response ---\n",
    "        model_response = call_model(prompt, prompt_configs, post_processing, model_obj, tokenizer_obj, debug=debug)\n",
    "        \n",
    "        # --- Calculate the correct answer for two numbers ---\n",
    "        answer = int_a + int_b\n",
    "        \n",
    "        # --- Append all results for analysis ---\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(0.1)\n",
    "\n",
    "    # --- Create a DataFrame to display the results for two numbers ---\n",
    "    df = pd.DataFrame({\n",
    "        'int_a': int_as, \n",
    "        'int_b': int_bs, \n",
    "        'prompt': prompts, \n",
    "        'answer': answers, \n",
    "        'response': model_responses, \n",
    "        'correct': correct\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "    # --- Calculate and return performance metrics ---\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum() / len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * (1 / prompt_length) * (1 - mae / (1 * 10**4))\n",
    "    \n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ba61360f254bb08c6a6a61621c471e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   int_a  int_b                             prompt  answer  response  correct\n",
      "0      7      4   Question: What is 7+4?\\nAnswer:       11        11     True\n",
      "1      2      2   Question: What is 2+2?\\nAnswer:        4         4     True\n",
      "2      9     10  Question: What is 9+10?\\nAnswer:       19        19     True\n",
      "3      7      8   Question: What is 7+8?\\nAnswer:       15        15     True\n",
      "4      6     10  Question: What is 6+10?\\nAnswer:       16        16     True\n",
      "5      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "6      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "7      8      3   Question: What is 8+3?\\nAnswer:       11        11     True\n",
      "8      9      6   Question: What is 9+6?\\nAnswer:       15        15     True\n",
      "9      4      5   Question: What is 4+5?\\nAnswer:        9         9     True\n",
      "{'res': np.float64(0.034482758620689655), 'acc': np.float64(1.0), 'mae': 0.0, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "# All of this remains the same\n",
    "added_prompt = ('Question: What is ', '?\\\\nAnswer: ')\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "# The model name string is no longer passed to the function\n",
    "# It was used in the previous cell to load the 'model' and 'tokenizer' objects\n",
    "print(f\"Testing model: {model_id_1}\")\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# This is the only line that changes\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt,\n",
    "    prompt_configs=prompt_config,\n",
    "    rng=rng,\n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer,\n",
    "    n_sample=10,\n",
    "    lower_bound=1,\n",
    "    upper_bound=10,\n",
    "    fixed_pairs=None,\n",
    "    pre_processing=your_pre_processing,\n",
    "    post_processing=your_post_processing,\n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2639efa7ae04e13983262a971121508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0  10155336    False  \n",
      "1   2517511     True  \n",
      "2  17530200    False  \n",
      "3   6459722    False  \n",
      "4   5892625    False  \n",
      "5   9367109    False  \n",
      "6   9998656    False  \n",
      "7   9647800    False  \n",
      "8  14639761    False  \n",
      "9   8501592     True  \n",
      "{'res': np.float64(-1.200898275862069), 'acc': np.float64(0.2), 'mae': 1751302.5, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# The call to test_range is updated to pass the model and tokenizer objects.\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty')\n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1c. Do 7-digit addition with Qwen3 8B.\n",
    "\n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 model offloaded and GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# --- Before loading Qwen 3, offload Llama 2 to free up VRAM ---\n",
    "\n",
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Llama 2 model offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for Qwen/Qwen3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f102468b450e45cc8b2d94cee2af826c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df36ed51026496d85b17dc0a21b5983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955c9f852d3443a3ba9a331c706c973f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e8d10326eb4b4bb621c4c6bd14fe42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5c8955d1ed4163a800d6160d4125b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9890a6d24c493ea8395cb45a0959a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e70050b01b8408d9a647a270d4daf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf6eac13b64bd1a590c7cae5200bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58be08151c6b4e2ca0535ffbe673f593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd80c5859adb4e35a23a5cff2ef1f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e729c48dc56c436684d3b1481073dbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f8c7ef3b7649c782b2489dfadb652f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494ea372914e41cda704ae633777cbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cd6056063e46a58d4b43af61528444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen3-8B model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load Qwen 3 8B ---\n",
    "# This is a different model, so we need to load its specific tokenizer and weights.\n",
    "model_id_2 = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer for {model_id_2}...\")\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2)\n",
    "\n",
    "print(f\"Loading model: {model_id_2}...\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_2,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_2} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 8, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1, 'stop': []}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694f28e9720d464b923b7605a85d65c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0  10160736     True  \n",
      "1   2517511     True  \n",
      "2  17534232     True  \n",
      "3  14025191     True  \n",
      "4  15308276     True  \n",
      "5   9367329     True  \n",
      "6  10018909     True  \n",
      "7  10147800     True  \n",
      "8  14641761     True  \n",
      "9   8499992    False  \n",
      "{'res': np.float64(0.030537931034482758), 'acc': np.float64(0.9), 'mae': 160.0, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "# --- Test on 7-digit addition ---\n",
    "prompt_config['max_tokens'] = 8\n",
    "\n",
    "print(prompt_config)\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71045ce01d0f46ce8a21a1ab2a5d598b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "         response  correct  \n",
      "0  10160736389247    False  \n",
      "1  25175111268762    False  \n",
      "2        17534232     True  \n",
      "3        14025191     True  \n",
      "4        15308276     True  \n",
      "5         9367329     True  \n",
      "6        10018909     True  \n",
      "7        10147800     True  \n",
      "8        14641761     True  \n",
      "9         8491592    False  \n",
      "{'res': np.float64(-8529339.45614945), 'acc': np.float64(0.7), 'mae': 3533583498976.2, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 20\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen3-8B offloaded and GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model_2\n",
    "del tokenizer_2\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"{model_id_2} offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "We will use [llama-2-7b]. Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n",
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e96efbe94064e1698b2466aa6ce1d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c045ed05d44a438f864a8f90ee882b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10160736  10150436    False  \n",
      "1   2517511   2517511     True  \n",
      "2  17534232  17533242    False  \n",
      "3  14025191   7322295    False  \n",
      "4  15308276  10308266    False  \n",
      "5   9367329   9367129    False  \n",
      "6  10018909   9624909    False  \n",
      "7  10147800   9647800    False  \n",
      "8  14641761  14640761    False  \n",
      "9   8501592   8502572    False  \n",
      "{'res': np.float64(-0.19857739682539685), 'acc': np.float64(0.1), 'mae': 1261037.6, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* Modify the post processing function\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your updated post processing function here\n",
    "def improved_post_processing(output_string):\n",
    "    \"\"\"\n",
    "    Extracts the numeric answer from the model's generated output.\n",
    "    Handles extra text, whitespace, or newlines.\n",
    "    \"\"\"\n",
    "    # Remove all non-digit characters\n",
    "    only_digits = re.findall(r'\\d+', output_string)\n",
    "    \n",
    "    if not only_digits:\n",
    "        # If no digits found, return 0 as fallback\n",
    "        return 0\n",
    "    \n",
    "    # Combine all digit groups into a single integer\n",
    "    res = int(''.join(only_digits))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64eb2ecf58cb45be818f5c91826a02ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer                         response  correct  \n",
      "0  10160736                         10150436    False  \n",
      "1   2517511                          2517511     True  \n",
      "2  17534232  1753328248257710592345678934567    False  \n",
      "3  14025191               732429164597227565    False  \n",
      "4  15308276           1030816627141345627429    False  \n",
      "5   9367329       93671292732884485362768457    False  \n",
      "6  10018909       96189894527183276710000132    False  \n",
      "7  10147800                         96478005    False  \n",
      "8  14641761                         14640761    False  \n",
      "9   8501592                          8502572    False  \n",
      "{'res': np.float64(-2.7833620801214594e+22), 'acc': np.float64(0.1), 'mae': 1.7535181104765195e+29, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ad3236e5b7413bb54998808302769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  1254878  2118550  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "1  7035620  6824705  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "2  6538466  4453098  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "3  9974889  9827518  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "4  7169878  6854133  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "5  7196020  4500293  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "6  2215869  7493395  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "7  5728189  3792177  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "8  5372518  9005390  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "9  9406391  4220157  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0   3373428   3373428     True  \n",
      "1  13860325  13850825    False  \n",
      "2  10991564  10982064    False  \n",
      "3  19802407  19802497    False  \n",
      "4  14024011  13923901    False  \n",
      "5  11696313  11696213    False  \n",
      "6   9709264   9612864    False  \n",
      "7   9520366   9520366     True  \n",
      "8  14377908  14377408    False  \n",
      "9  13626548   5626548    False  \n",
      "{'res': np.float64(-0.205473417721519), 'acc': np.float64(0.2), 'mae': 821620.0, 'prompt_length': 79}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+1234567?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b8e4e1c99a43ae9527c2556bb51bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6143768  3896825  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "1  6348700  4041201  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "2  4524571  9012469  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "3  3044419  6608684  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "4  1756139  8493797  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "5  8083884  3154325  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "6  8888358  1527113  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "7  4025054  2352516  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "8  5053054  8166918  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "9  3075780  1468192  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer                          response  correct  \n",
      "0  10040593  10034543924537623456781159105475    False  \n",
      "1  10389901  10389301876543276543216419654987    False  \n",
      "2  13537040  54360407890162234567810234740345    False  \n",
      "3   9653103  96490045678907856789014246797987    False  \n",
      "4  10249936  10257176987654321987130983145678    False  \n",
      "5  11238209  11237109987654345678901444043376    False  \n",
      "6  10415471  10458493456789234567857924679876    False  \n",
      "7   6377570  63775607890113210981009109987654    False  \n",
      "8  13219972  13189076987654323456781222302176    False  \n",
      "9   4543972  45440726789016234567891347948579    False  \n",
      "{'res': np.float64(-0.0), 'acc': np.float64(0.0), 'mae': 3.256324914690327e+31, 'prompt_length': 79}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 50 \n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d96ce392ac4761b2e296e7264ee964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 9090909+1010101?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "1111000\n",
      "Question: What is 3456789+2345678?\n",
      "Answer: 5792467\n",
      "Question: What is 4567\n",
      "\n",
      "************ Final Output ************\n",
      "11110003456789234567857924674567\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  9090909  1010101  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer                          response  correct  \n",
      "0  10101010  11110003456789234567857924674567    False  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res': np.float64(-0.0),\n",
       " 'acc': np.float64(0.0),\n",
       " 'mae': 1.1110003456789234e+31,\n",
       " 'prompt_length': 79}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    fixed_pairs=[(9090909,1010101)], \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer, \n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
